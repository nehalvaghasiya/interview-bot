# LLM Provider Configuration
# Choose your provider: 'openai' or 'ollama'
LLM_PROVIDER=openai

# OpenAI Configuration (when LLM_PROVIDER=openai)
OPENAI_API_KEY=your_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini

# Ollama Configuration (when LLM_PROVIDER=ollama)
# Make sure Ollama is running locally: ollama serve
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama3.2
# Ollama doesn't require an API key, but we set a placeholder
OLLAMA_API_KEY=ollama